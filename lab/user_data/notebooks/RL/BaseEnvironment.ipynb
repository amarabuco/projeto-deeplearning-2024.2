{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from abc import abstractmethod\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from pandas import DataFrame\n",
    "\n",
    "from freqtrade.exceptions import OperationalException\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseActions(Enum):\n",
    "    \"\"\"\n",
    "    Default action space, mostly used for type handling.\n",
    "    \"\"\"\n",
    "\n",
    "    Neutral = 0\n",
    "    Long_enter = 1\n",
    "    Long_exit = 2\n",
    "    Short_enter = 3\n",
    "    Short_exit = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Neutral = 0.5\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Base class for environments. This class is agnostic to action count.\n",
    "    Inherited classes customize this to include varying action counts/types,\n",
    "    See RL/Base5ActionRLEnv.py and RL/Base4ActionRLEnv.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: DataFrame = DataFrame(),\n",
    "        prices: DataFrame = DataFrame(),\n",
    "        reward_kwargs: dict = {},\n",
    "        window_size=10,\n",
    "        starting_point=True,\n",
    "        id: str = \"baseenv-1\",\n",
    "        seed: int = 1,\n",
    "        config: dict = {},\n",
    "        live: bool = False,\n",
    "        fee: float = 0.0015,\n",
    "        can_short: bool = False,\n",
    "        pair: str = \"\",\n",
    "        df_raw: DataFrame = DataFrame(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the training/eval environment.\n",
    "        :param df: dataframe of features\n",
    "        :param prices: dataframe of prices to be used in the training environment\n",
    "        :param window_size: size of window (temporal) to pass to the agent\n",
    "        :param reward_kwargs: extra config settings assigned by user in `rl_config`\n",
    "        :param starting_point: start at edge of window or not\n",
    "        :param id: string id of the environment (used in backend for multiprocessed env)\n",
    "        :param seed: Sets the seed of the environment higher in the gym.Env object\n",
    "        :param config: Typical user configuration file\n",
    "        :param live: Whether or not this environment is active in dry/live/backtesting\n",
    "        :param fee: The fee to use for environmental interactions.\n",
    "        :param can_short: Whether or not the environment can short\n",
    "        \"\"\"\n",
    "        self.config: dict = config\n",
    "        self.rl_config: dict = config[\"freqai\"][\"rl_config\"]\n",
    "        self.add_state_info: bool = self.rl_config.get(\"add_state_info\", False)\n",
    "        self.id: str = id\n",
    "        self.max_drawdown: float = 1 - \\\n",
    "            self.rl_config.get(\"max_training_drawdown_pct\", 0.8)\n",
    "        self.compound_trades: bool = config[\"stake_amount\"] == \"unlimited\"\n",
    "        self.pair: str = pair\n",
    "        self.raw_features: DataFrame = df_raw\n",
    "        if self.config.get(\"fee\", None) is not None:\n",
    "            self.fee = self.config[\"fee\"]\n",
    "        else:\n",
    "            self.fee = fee\n",
    "\n",
    "        # set here to default 5Ac, but all children envs can override this\n",
    "        self.actions: Type[Enum] = BaseActions  # BaseActions é 5Ac\n",
    "        self.tensorboard_metrics: dict = {}\n",
    "        self.can_short: bool = can_short\n",
    "        self.live: bool = live\n",
    "        if not self.live and self.add_state_info:\n",
    "            raise OperationalException(\n",
    "                \"`add_state_info` is not available in backtesting. Change \"\n",
    "                \"parameter to false in your rl_config. See `add_state_info` \"\n",
    "                \"docs for more info.\"\n",
    "            )\n",
    "        self.seed(seed)\n",
    "        self.reset_env(df, prices, window_size, reward_kwargs, starting_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reset_env(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        prices: DataFrame,\n",
    "        window_size: int,\n",
    "        reward_kwargs: dict,\n",
    "        starting_point=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Resets the environment when the agent fails (in our case, if the drawdown\n",
    "        exceeds the user set max_training_drawdown_pct)\n",
    "        :param df: dataframe of features\n",
    "        :param prices: dataframe of prices to be used in the training environment\n",
    "        :param window_size: size of window (temporal) to pass to the agent\n",
    "        :param reward_kwargs: extra config settings assigned by user in `rl_config`\n",
    "        :param starting_point: start at edge of window or not\n",
    "        \"\"\"\n",
    "        self.signal_features: DataFrame = df\n",
    "        self.prices: DataFrame = prices\n",
    "        self.window_size: int = window_size\n",
    "        self.starting_point: bool = starting_point\n",
    "        self.rr: float = reward_kwargs[\"rr\"]\n",
    "        self.profit_aim: float = reward_kwargs[\"profit_aim\"]\n",
    "\n",
    "        # # spaces\n",
    "        if self.add_state_info:\n",
    "            self.total_features = self.signal_features.shape[1] + 3\n",
    "        else:\n",
    "            self.total_features = self.signal_features.shape[1]\n",
    "        self.shape = (window_size, self.total_features)\n",
    "        self.set_action_space()\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, high=1, shape=self.shape, dtype=np.float32)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick: int = self.window_size\n",
    "        self._end_tick: int = len(self.prices) - 1\n",
    "        self._done: bool = False\n",
    "        self._current_tick: int = self._start_tick\n",
    "        self._last_trade_tick: Optional[int] = None\n",
    "        self._position = Positions.Neutral\n",
    "        self._position_history: list = [None]\n",
    "        self.total_reward: float = 0\n",
    "        self._total_profit: float = 1\n",
    "        self._total_unrealized_profit: float = 1\n",
    "        self.history: dict = {}\n",
    "        self.trade_history: list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_attr(self, attr: str):\n",
    "        \"\"\"\n",
    "        Returns the attribute of the environment\n",
    "        :param attr: attribute to return\n",
    "        :return: attribute\n",
    "        \"\"\"\n",
    "        return getattr(self, attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código define um método reset_env para redefinir o ambiente de uma simulação, geralmente utilizado em algoritmos de aprendizado por reforço (Reinforcement Learning, RL) no contexto de negociação financeira. Esse método é projetado para resetar variáveis e parâmetros do ambiente quando o agente falha ou atinge uma condição de parada, como um drawdown (perda máxima) excessivo. Vamos entender cada parte:\n",
    "\n",
    "Parâmetros do método\n",
    "df: DataFrame: DataFrame com as features (ou variáveis de entrada) que o agente utilizará para tomar decisões.\n",
    "prices: DataFrame: DataFrame contendo os preços dos ativos no ambiente de treino, importante para calcular retornos, lucros, etc.\n",
    "window_size: int: Tamanho da janela temporal usada para apresentar dados ao agente. Um valor de 10, por exemplo, significa que o agente observa 10 períodos anteriores a cada ação.\n",
    "reward_kwargs: dict: Dicionário de configurações adicionais para o cálculo de recompensa (parâmetros personalizados passados pelo usuário).\n",
    "starting_point: bool: Define se o ponto de início da janela de observação está na borda do conjunto de dados (True).\n",
    "Corpo do Método\n",
    "Definição de variáveis e parâmetros iniciais:\n",
    "\n",
    "self.signal_features: Atribui as features ao ambiente.\n",
    "self.prices: Define os preços para o ambiente.\n",
    "self.window_size: Armazena o tamanho da janela temporal.\n",
    "self.starting_point: Define se deve começar na borda ou não.\n",
    "self.rr e self.profit_aim: Extraem valores de recompensa do dicionário reward_kwargs.\n",
    "Definição dos espaços:\n",
    "\n",
    "self.total_features: Total de features considerando informações adicionais de estado, caso self.add_state_info esteja ativado (como features extras para o estado do agente).\n",
    "self.shape: Forma da janela observada pelo agente, que depende de window_size e do total de features.\n",
    "self.set_action_space(): Chama um método (presumivelmente) para configurar o espaço de ações, que define quais ações o agente pode tomar.\n",
    "self.observation_space: Define o espaço de observação (ou espaço de estados), com limites entre -1 e 1, baseado em window_size e total_features.\n",
    "Inicialização do episódio:\n",
    "\n",
    "Define variáveis para gerenciar o estado do episódio:\n",
    "self._start_tick: O primeiro tick observável com base em window_size.\n",
    "self._end_tick: O último tick observável, baseado no tamanho de prices.\n",
    "self._done: Variável booleana para sinalizar o término do episódio.\n",
    "self._current_tick: Marca o tick atual no episódio.\n",
    "self._last_trade_tick: Guarda o tick da última transação (inicialmente None).\n",
    "self._position: Define a posição inicial do agente como Neutral.\n",
    "self._position_history: Histórico das posições do agente.\n",
    "self.total_reward: Recompensa acumulada durante o episódio.\n",
    "self._total_profit e self._total_unrealized_profit: Lucro total e lucro não realizado acumulados.\n",
    "self.history: Dicionário para guardar o histórico completo do episódio.\n",
    "self.trade_history: Lista para guardar o histórico de transações.\n",
    "Esse método é essencial para reinicializar o ambiente para que o agente possa começar um novo episódio de treino com condições iniciais específicas, mantendo o ambiente consistente e preparado para realizar novas decisões de negociação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@abstractmethod\n",
    "    def set_action_space(self):\n",
    "        \"\"\"\n",
    "        Unique to the environment action count. Must be inherited.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O decorador @abstractmethod indica que set_action_space é um método abstrato, ou seja, ele precisa ser implementado em qualquer classe que herde da classe onde esse método está definido.\n",
    "\n",
    "Detalhamento\n",
    "@abstractmethod: Esse decorador é usado para marcar métodos em uma classe abstrata (normalmente uma classe que herda de ABC do módulo abc). Classes abstratas servem como modelos para outras classes. Elas não podem ser instanciadas diretamente e contêm métodos que devem ser obrigatoriamente implementados pelas classes filhas.\n",
    "\n",
    "Método set_action_space: Este método define o espaço de ação para o ambiente. Em um contexto de aprendizado por reforço, o espaço de ação representa todas as ações possíveis que o agente pode realizar no ambiente. Cada ambiente de aprendizado por reforço pode ter um conjunto específico de ações (por exemplo, comprar, vender, manter posição, etc.), e set_action_space é onde esse conjunto é configurado.\n",
    "\n",
    "Must be inherited: A docstring (\"\"\"Unique to the environment action count. Must be inherited.\"\"\") indica que o método set_action_space deve ser implementado especificamente em cada ambiente que herda desta classe. Isso permite que cada ambiente tenha sua própria contagem e tipo de ações, adaptadas às suas necessidades particulares.\n",
    "\n",
    "Exemplo de Uso\n",
    "Ao implementar uma classe filha que herda dessa classe base, você deve definir set_action_space. Por exemplo:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "class TradingEnv(CustomEnvBase):\n",
    "    def set_action_space(self):\n",
    "        # Define um espaço de ação específico para o ambiente de negociação\n",
    "        self.action_space = spaces.Discrete(3)  # Exemplo: 3 ações possíveis (comprar, vender, manter)\n",
    "Neste exemplo, a implementação do método set_action_space na classe TradingEnv define o espaço de ações como um espaço discreto com 3 ações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def action_masks(self) -> List[bool]:\n",
    "        return [self._is_valid(action.value) for action in self.actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método action_masks retorna uma lista de valores booleanos, onde cada valor indica se uma ação é válida ou não no momento atual. Vamos entender cada parte:\n",
    "\n",
    "Análise do Método action_masks\n",
    "Retorno do Método:\n",
    "\n",
    "O método retorna uma lista de valores booleanos (List[bool]), onde cada elemento da lista é o resultado da verificação de uma ação específica.\n",
    "Lista de Ações:\n",
    "\n",
    "O método percorre self.actions, que parece ser uma lista de ações possíveis no ambiente. Cada ação provavelmente é uma instância de uma classe que tem um atributo value, representando o valor específico ou o identificador da ação.\n",
    "Validação das Ações com _is_valid:\n",
    "\n",
    "Para cada action em self.actions, o método chama self._is_valid(action.value).\n",
    "Esse _is_valid é presumivelmente um método auxiliar que verifica se uma determinada ação (identificada por action.value) é permitida ou válida no estado atual do ambiente.\n",
    "A implementação de _is_valid não está visível aqui, mas é provável que ele contenha alguma lógica condicional baseada no estado do ambiente ou em restrições específicas.\n",
    "Exemplo de Uso:\n",
    "\n",
    "Esse método é frequentemente utilizado em ambientes de aprendizado por reforço onde algumas ações podem não estar disponíveis ou recomendadas em certos estados.\n",
    "Ao retornar uma lista de máscaras de ações (True para ações válidas e False para inválidas), o ambiente pode restringir o agente a realizar apenas ações que fazem sentido em um determinado estado.\n",
    "Exemplo Prático\n",
    "Suponha que estamos em um ambiente de negociação, onde o agente pode comprar, vender ou manter posição. Em algumas situações, algumas ações podem não ser permitidas:\n",
    "\n",
    "Se o agente já comprou e está com uma posição aberta, a ação de comprar novamente pode ser inválida.\n",
    "Da mesma forma, vender sem ter uma posição aberta também seria inválido.\n",
    "Nesse caso, o método action_masks pode retornar algo como:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "[True, False, True]  # Onde \"True\" representa uma ação válida e \"False\" uma inválida.\n",
    "Resumo\n",
    "O método action_masks fornece uma forma eficiente de informar ao agente quais ações são viáveis em um determinado momento, evitando que ele realize ações inválidas. Ele é útil para algoritmos de aprendizado por reforço que precisam filtrar ações disponíveis em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def seed(self, seed: int = 1):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboard_log(\n",
    "        self,\n",
    "        metric: str,\n",
    "        value: Optional[Union[int, float]] = None,\n",
    "        inc: Optional[bool] = None,\n",
    "        category: str = \"custom\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function builds the tensorboard_metrics dictionary\n",
    "        to be parsed by the TensorboardCallback. This\n",
    "        function is designed for tracking incremented objects,\n",
    "        events, actions inside the training environment.\n",
    "        For example, a user can call this to track the\n",
    "        frequency of occurrence of an `is_valid` call in\n",
    "        their `calculate_reward()`:\n",
    "\n",
    "        def calculate_reward(self, action: int) -> float:\n",
    "            if not self._is_valid(action):\n",
    "                self.tensorboard_log(\"invalid\")\n",
    "                return -2\n",
    "\n",
    "        :param metric: metric to be tracked and incremented\n",
    "        :param value: `metric` value\n",
    "        :param inc: (deprecated) sets whether the `value` is incremented or not\n",
    "        :param category: `metric` category\n",
    "        \"\"\"\n",
    "        increment = True if value is None else False\n",
    "        value = 1 if increment else value\n",
    "\n",
    "        if category not in self.tensorboard_metrics:\n",
    "            self.tensorboard_metrics[category] = {}\n",
    "\n",
    "        if not increment or metric not in self.tensorboard_metrics[category]:\n",
    "            self.tensorboard_metrics[category][metric] = value\n",
    "        else:\n",
    "            self.tensorboard_metrics[category][metric] += value\n",
    "\n",
    "    def reset_tensorboard_log(self):\n",
    "        self.tensorboard_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self, seed=None):\n",
    "    \"\"\"\n",
    "        Reset is called at the beginning of every episode\n",
    "        \"\"\"\n",
    "     self.reset_tensorboard_log()\n",
    "\n",
    "      self._done = False\n",
    "\n",
    "       if self.starting_point is True:\n",
    "            if self.rl_config.get(\"randomize_starting_position\", False):\n",
    "                length_of_data = int(self._end_tick / 4)\n",
    "                start_tick = random.randint(\n",
    "                    self.window_size + 1, length_of_data)\n",
    "                self._start_tick = start_tick\n",
    "            self._position_history = (\n",
    "                self._start_tick * [None]) + [self._position]\n",
    "        else:\n",
    "            self._position_history = (\n",
    "                self.window_size * [None]) + [self._position]\n",
    "\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = None\n",
    "        self._position = Positions.Neutral\n",
    "\n",
    "        self.total_reward = 0.0\n",
    "        self._total_profit = 1.0  # unit\n",
    "        self.history = {}\n",
    "        self.trade_history = []\n",
    "        self.portfolio_log_returns = np.zeros(len(self.prices))\n",
    "\n",
    "        self._profits = [(self._start_tick, 1)]\n",
    "        self.close_trade_profit = []\n",
    "        self._total_unrealized_profit = 1\n",
    "\n",
    "        return self._get_observation(), self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método reset redefine o ambiente no início de cada episódio, restaurando variáveis e estados para iniciar uma nova sequência de interações. Em aprendizado por reforço, isso é essencial para garantir que cada episódio comece com uma condição inicial consistente ou aleatorizada, conforme necessário. Vamos detalhar cada parte do código:\n",
    "\n",
    "Estrutura do Método reset\n",
    "self.reset_tensorboard_log():\n",
    "\n",
    "Este método parece redefinir os logs no TensorBoard (uma ferramenta de visualização para o treinamento de redes neurais), garantindo que os dados de logs do episódio anterior não interfiram no novo episódio.\n",
    "Definição do Estado Inicial:\n",
    "\n",
    "self._done = False: Define que o episódio não está concluído (False). Esta variável será utilizada para indicar quando o episódio termina.\n",
    "Inicialização do start_tick:\n",
    "\n",
    "self.starting_point: Dependendo do valor de self.starting_point, a posição inicial (_start_tick) pode ser escolhida de duas formas:\n",
    "Se self.rl_config.get(\"randomize_starting_position\", False) for True, o início é aleatório dentro de um intervalo que evita o começo exato dos dados. Isso ajuda o agente a treinar em diferentes partes da série temporal.\n",
    "Caso contrário, define-se uma posição de início fixa, ou no início do intervalo (window_size), se starting_point for False.\n",
    "Histórico de Posições:\n",
    "\n",
    "self._position_history: Inicializa o histórico de posições, registrando o estado inicial como None até o _start_tick. Isso é útil para acompanhar as posições do agente ao longo do episódio.\n",
    "Outras Variáveis de Estado:\n",
    "\n",
    "self._current_tick: Define o tick atual como _start_tick, indicando o ponto de início do episódio.\n",
    "self._last_trade_tick = None: Inicializa o último tick de negociação como None, pois ainda não houve negociações no início do episódio.\n",
    "self._position = Positions.Neutral: Define a posição inicial do agente como Neutral, indicando que ele não possui uma posição de negociação ativa no início.\n",
    "Inicialização de Métricas e Logs:\n",
    "\n",
    "self.total_reward = 0.0: Define a recompensa total acumulada no episódio como 0.0.\n",
    "self._total_profit = 1.0: Inicializa o lucro total como 1.0 (provavelmente um valor neutro, onde um lucro multiplicativo aumentaria esse valor).\n",
    "self.history = {} e self.trade_history = []: Define dicionários ou listas vazias para armazenar o histórico de estados e transações durante o episódio.\n",
    "self.portfolio_log_returns = np.zeros(len(self.prices)): Cria um array de zeros do tamanho de prices, que armazenará os retornos logarítmicos do portfólio ao longo do tempo.\n",
    "Lucros e Negociações Fechadas:\n",
    "\n",
    "self._profits = [(self._start_tick, 1)]: Inicializa uma lista de lucros onde o primeiro valor é o start_tick e 1, indicando um lucro inicial neutro.\n",
    "self.close_trade_profit = []: Lista para armazenar os lucros de negociações fechadas.\n",
    "self._total_unrealized_profit = 1: Define o lucro não realizado como 1, representando o ponto de partida sem lucros ou perdas acumuladas.\n",
    "Retorno da Observação Inicial e Histórico:\n",
    "\n",
    "return self._get_observation(), self.history: Retorna a primeira observação (self._get_observation()) e o histórico inicial vazio. A primeira observação será o estado que o agente usará como entrada para decidir sua primeira ação.\n",
    "Resumo\n",
    "Esse método prepara o ambiente para um novo episódio, reiniciando todas as variáveis críticas, incluindo o histórico de transações, lucros, recompensas acumuladas e posição atual. Isso garante que cada episódio comece em um estado limpo e consistente, o que é essencial para o aprendizado por reforço, especialmente ao treinar e avaliar o desempenho do agente em diferentes condições do ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As diferenças entre os métodos reset_env e reset estão na função e na finalidade de cada um no contexto de um ambiente de aprendizado por reforço. Vamos detalhar essas diferenças:\n",
    "\n",
    "1. Propósito Geral\n",
    "reset_env: Esse método é voltado para redefinir o ambiente em termos de configuração inicial e variáveis globais que controlam a simulação. Ele configura as variáveis essenciais do ambiente, como dados de preços, recursos de sinal, tamanho da janela, e parâmetros de recompensa. Esse método é mais sobre preparar o ambiente de forma completa e é chamado antes de qualquer episódio ser iniciado.\n",
    "\n",
    "reset: Esse método é chamado no início de cada episódio e redefine apenas as variáveis necessárias para o episódio atual. Ele prepara o ambiente para o começo do episódio, reiniciando variáveis como o tick inicial, histórico de posições, total de lucros e recompensas acumuladas. O reset assume que o ambiente já foi configurado pelo reset_env e apenas coloca o agente no ponto de partida do episódio.\n",
    "\n",
    "2. Configuração do Ambiente vs. Preparação para o Episódio\n",
    "reset_env: É responsável por configurar o ambiente inteiro (por exemplo, os dados de entrada, espaço de observação, espaço de ação). Pode incluir:\n",
    "Definir o espaço de observação e de ação (exemplo: self.observation_space e self.action_space).\n",
    "Configurar variáveis globais que não mudam a cada episódio, como dados históricos de preços ou variáveis de configuração (como reward_kwargs).\n",
    "Esse método pode ser chamado uma vez por simulação ou em casos específicos onde há necessidade de uma reconfiguração completa do ambiente.\n",
    "reset: Prepara o ambiente apenas para o episódio atual. Isso inclui:\n",
    "Inicializar ou reiniciar variáveis de estado e métricas do episódio, como total_reward, total_profit, e position_history.\n",
    "Definir a posição inicial do agente (por exemplo, start_tick).\n",
    "Este método é chamado no início de cada episódio, garantindo que o agente comece de um estado limpo e consistente para cada nova execução.\n",
    "3. Exemplo de Fluxo de Uso\n",
    "Em um fluxo de treinamento típico:\n",
    "\n",
    "reset_env é chamado para configurar o ambiente com dados e variáveis globais. Isso ocorre apenas uma vez antes do início dos episódios, ou quando há necessidade de uma reconfiguração completa.\n",
    "\n",
    "Para cada novo episódio:\n",
    "\n",
    "O método reset é chamado para reinicializar as variáveis específicas do episódio, como a posição do agente, as recompensas acumuladas, e os lucros iniciais.\n",
    "O episódio então é executado, com o agente realizando ações e o ambiente respondendo a essas ações.\n",
    "Ao final do episódio, o ambiente pode chamar reset novamente para preparar o próximo episódio.\n",
    "Resumo\n",
    "reset_env: Configuração inicial e global do ambiente, chamada uma vez por simulação ou quando o ambiente precisa ser redefinido por completo.\n",
    "reset: Preparação específica para cada episódio, garantindo que o agente comece cada um com um estado limpo e as variáveis do episódio reinicializadas.\n",
    "Essa divisão ajuda a manter o código organizado e eficiente, pois reset_env lida com a configuração mais pesada e reset apenas prepara o ambiente para um novo episódio sem reconfigurar tudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@abstractmethod\n",
    "    def step(self, action: int):\n",
    "        \"\"\"\n",
    "        Step depends on action types, this must be inherited.\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        This may or may not be independent of action types, user can inherit\n",
    "        this in their custom \"MyRLEnv\"\n",
    "        \"\"\"\n",
    "        features_window = self.signal_features[\n",
    "            (self._current_tick - self.window_size): self._current_tick\n",
    "        ]\n",
    "        if self.add_state_info:\n",
    "            features_and_state = DataFrame(\n",
    "                np.zeros((len(features_window), 3)),\n",
    "                columns=[\"current_profit_pct\", \"position\", \"trade_duration\"],\n",
    "                index=features_window.index,\n",
    "            )\n",
    "\n",
    "            features_and_state[\"current_profit_pct\"] = self.get_unrealized_profit(\n",
    "            )\n",
    "            features_and_state[\"position\"] = self._position.value\n",
    "            features_and_state[\"trade_duration\"] = self.get_trade_duration()\n",
    "            features_and_state = pd.concat(\n",
    "                [features_window, features_and_state], axis=1)\n",
    "            return features_and_state\n",
    "        else:\n",
    "            return features_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método _get_observation é responsável por construir e retornar a observação atual do ambiente, ou seja, o estado que o agente \"vê\" no momento. Esse método inclui um conjunto de variáveis (ou \"features\") sobre o ambiente e, opcionalmente, algumas informações adicionais de estado.\n",
    "\n",
    "Detalhamento do Método\n",
    "Construção da Janela de Features:\n",
    "\n",
    "features_window: Este trecho seleciona uma janela de dados das signal_features (variáveis de entrada, como indicadores de mercado). A janela tem tamanho self.window_size e termina no self._current_tick, ou seja, o ponto atual no episódio.\n",
    "Seleção da Janela: Esse recorte (self._current_tick - self.window_size): self._current_tick gera uma sequência dos dados mais recentes que o agente usa como contexto.\n",
    "Adição de Informações de Estado (Condicional):\n",
    "\n",
    "self.add_state_info: Se essa variável estiver definida como True, o método adiciona informações de estado adicionais ao conjunto de observação, como:\n",
    "current_profit_pct: Calcula o lucro não realizado atual usando o método get_unrealized_profit().\n",
    "position: Indica a posição atual do agente (por exemplo, comprada, vendida ou neutra), acessada através de self._position.value.\n",
    "trade_duration: Calcula a duração da negociação atual (ou seja, o número de ticks desde que a posição foi aberta) usando get_trade_duration().\n",
    "Essas informações são adicionadas a um DataFrame features_and_state, que é então concatenado com features_window para criar uma visão mais ampla do estado.\n",
    "Concatenação de Dados:\n",
    "\n",
    "O features_and_state é criado como um novo DataFrame que contém as três colunas adicionais: current_profit_pct, position, e trade_duration. Ele é concatenado com features_window ao longo do eixo das colunas (axis=1), formando um único DataFrame com todas as features, incluindo as informações adicionais de estado.\n",
    "Retorno da Observação:\n",
    "\n",
    "Quando add_state_info é True: Retorna o DataFrame concatenado com as features e as informações de estado.\n",
    "Quando add_state_info é False: Retorna apenas features_window, sem as informações adicionais.\n",
    "Finalidade do Método\n",
    "Esse método é configurável para fornecer ao agente observações personalizadas. Em algumas aplicações de aprendizado por reforço, informações adicionais de estado podem ser úteis para aumentar o contexto que o agente usa para decidir ações, enquanto em outras, manter o estado simples (apenas as features de entrada) é preferível. Essa flexibilidade permite que o ambiente seja adaptado para diferentes tipos de agentes e experimentos de RL.\n",
    "\n",
    "Exemplo de Uso\n",
    "Se o window_size for 10, e o current_tick for 15, a observação retornada incluirá os dados dos ticks 5 a 15 nas signal_features, com ou sem as informações adicionais de estado, dependendo da configuração.\n",
    "\n",
    "Esse design permite que o método _get_observation seja herdado e modificado em uma classe customizada (MyRLEnv), dando ao usuário a possibilidade de ajustar o tipo de observação que o agente recebe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_trade_duration(self):\n",
    "        \"\"\"\n",
    "        Get the trade duration if the agent is in a trade\n",
    "        \"\"\"\n",
    "        if self._last_trade_tick is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return self._current_tick - self._last_trade_tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def get_unrealized_profit(self):\n",
    "        \"\"\"\n",
    "        Get the unrealized profit if the agent is in a trade\n",
    "        \"\"\"\n",
    "        if self._last_trade_tick is None:\n",
    "            return 0.0\n",
    "\n",
    "        if self._position == Positions.Neutral:\n",
    "            return 0.0\n",
    "        elif self._position == Positions.Short:\n",
    "            current_price = self.add_entry_fee(\n",
    "                self.prices.iloc[self._current_tick].open)\n",
    "            last_trade_price = self.add_exit_fee(\n",
    "                self.prices.iloc[self._last_trade_tick].open)\n",
    "            return (last_trade_price - current_price) / last_trade_price\n",
    "        elif self._position == Positions.Long:\n",
    "            current_price = self.add_exit_fee(\n",
    "                self.prices.iloc[self._current_tick].open)\n",
    "            last_trade_price = self.add_entry_fee(\n",
    "                self.prices.iloc[self._last_trade_tick].open)\n",
    "            return (current_price - last_trade_price) / last_trade_price\n",
    "        else:\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    @abstractmethod\n",
    "    def is_tradesignal(self, action: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the signal is a trade signal. This is\n",
    "        unique to the actions in the environment, and therefore must be\n",
    "        inherited.\n",
    "        \"\"\"\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _is_valid(self, action: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if the signal is valid.This is\n",
    "        unique to the actions in the environment, and therefore must be\n",
    "        inherited.\n",
    "        \"\"\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def add_entry_fee(self, price):\n",
    "        return price * (1 + self.fee)\n",
    "\n",
    "    def add_exit_fee(self, price):\n",
    "        return price / (1 + self.fee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_history(self, info):\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @abstractmethod\n",
    "    def calculate_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        An example reward function. This is the one function that users will likely\n",
    "        wish to inject their own creativity into.\n",
    "\n",
    "        Warning!\n",
    "        This is function is a showcase of functionality designed to show as many possible\n",
    "        environment control features as possible. It is also designed to run quickly\n",
    "        on small computers. This is a benchmark, it is *not* for live production.\n",
    "\n",
    "        :param action: int = The action made by the agent for the current candle.\n",
    "        :return:\n",
    "        float = the reward to give to the agent for current step (used for optimization\n",
    "            of weights in NN)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _update_unrealized_total_profit(self):\n",
    "        \"\"\"\n",
    "        Update the unrealized total profit in case of episode end.\n",
    "        \"\"\"\n",
    "        if self._position in (Positions.Long, Positions.Short):\n",
    "            pnl = self.get_unrealized_profit()\n",
    "            if self.compound_trades:\n",
    "                # assumes unit stake and compounding\n",
    "                unrl_profit = self._total_profit * (1 + pnl)\n",
    "            else:\n",
    "                # assumes unit stake and no compounding\n",
    "                unrl_profit = self._total_profit + pnl\n",
    "            self._total_unrealized_profit = unrl_profit\n",
    "\n",
    "    def _update_total_profit(self):\n",
    "        pnl = self.get_unrealized_profit()\n",
    "        if self.compound_trades:\n",
    "            # assumes unit stake and compounding\n",
    "            self._total_profit = self._total_profit * (1 + pnl)\n",
    "        else:\n",
    "            # assumes unit stake and no compounding\n",
    "            self._total_profit += pnl\n",
    "\n",
    "    def current_price(self) -> float:\n",
    "        return self.prices.iloc[self._current_tick].open\n",
    "\n",
    "    def get_actions(self) -> Type[Enum]:\n",
    "        \"\"\"\n",
    "        Used by SubprocVecEnv to get actions from\n",
    "        initialized env for tensorboard callback\n",
    "        \"\"\"\n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Keeping around in case we want to start building more complex environment\n",
    "    # templates in the future.\n",
    "    def most_recent_return(self):\n",
    "        \"\"\"\n",
    "        Calculate the tick to tick return if in a trade.\n",
    "        Return is generated from rising prices in Long\n",
    "        and falling prices in Short positions.\n",
    "        The actions Sell/Buy or Hold during a Long position trigger the sell/buy-fee.\n",
    "        \"\"\"\n",
    "        # Long positions\n",
    "        if self._position == Positions.Long:\n",
    "            current_price = self.prices.iloc[self._current_tick].open\n",
    "            previous_price = self.prices.iloc[self._current_tick - 1].open\n",
    "\n",
    "            if (self._position_history[self._current_tick - 1] == Positions.Short\n",
    "                    or self._position_history[self._current_tick - 1] == Positions.Neutral):\n",
    "                previous_price = self.add_entry_fee(previous_price)\n",
    "\n",
    "            return np.log(current_price) - np.log(previous_price)\n",
    "\n",
    "        # Short positions\n",
    "        if self._position == Positions.Short:\n",
    "            current_price = self.prices.iloc[self._current_tick].open\n",
    "            previous_price = self.prices.iloc[self._current_tick - 1].open\n",
    "            if (self._position_history[self._current_tick - 1] == Positions.Long\n",
    "                    or self._position_history[self._current_tick - 1] == Positions.Neutral):\n",
    "                previous_price = self.add_exit_fee(previous_price)\n",
    "\n",
    "            return np.log(previous_price) - np.log(current_price)\n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método most_recent_return calcula o retorno mais recente com base no movimento de preços entre ticks consecutivos, levando em consideração a posição do agente (Long ou Short) e aplicando taxas de entrada ou saída quando necessário. Esse retorno é calculado como um retorno logarítmico, que é uma forma comum de representar mudanças percentuais em aprendizado por reforço financeiro.\n",
    "\n",
    "Detalhamento do Método most_recent_return\n",
    "Contexto de Posições:\n",
    "\n",
    "O método considera dois tipos de posições:\n",
    "Long (compra): O agente ganha com a valorização do preço.\n",
    "Short (venda): O agente ganha com a queda do preço.\n",
    "Quando o agente não está em uma posição (Neutral), o retorno é 0, pois não há variação acumulada.\n",
    "Cálculo para Posições Long:\n",
    "\n",
    "Se a posição atual (self._position) é Long:\n",
    "current_price e previous_price são os preços de abertura (open) nos ticks atual e anterior.\n",
    "Taxa de Entrada: Se a posição anterior era Short ou Neutral, o agente está iniciando uma nova posição Long, e uma taxa de entrada é aplicada ao preço anterior (previous_price = self.add_entry_fee(previous_price)).\n",
    "O retorno é calculado como a diferença logarítmica entre current_price e previous_price, ou seja: \\text{return} = \\log(\\text{current_price}) - \\log(\\text{previous_price})\n",
    "Cálculo para Posições Short:\n",
    "\n",
    "Se a posição atual é Short:\n",
    "current_price e previous_price são obtidos da mesma forma.\n",
    "Taxa de Saída: Se a posição anterior era Long ou Neutral, o agente está iniciando uma nova posição Short, e uma taxa de saída é aplicada ao preço anterior (previous_price = self.add_exit_fee(previous_price)).\n",
    "O retorno é então a diferença logarítmica invertida: \\text{return} = \\log(\\text{previous_price}) - \\log(\\text{current_price})\n",
    "Esse cálculo reflete o ganho do agente em uma posição Short, onde ele se beneficia da queda de preços.\n",
    "Retorno para Posições Neutras:\n",
    "\n",
    "Se o agente está em posição Neutral (nenhuma posição aberta), o método retorna 0, indicando que não há lucro ou perda quando o agente não está em uma posição de negociação.\n",
    "Observações Adicionais\n",
    "Retorno Logarítmico: A utilização do logaritmo natural para calcular o retorno é comum, pois ele fornece uma maneira aditiva de acumular retornos ao longo do tempo, o que facilita a análise e o treinamento do agente.\n",
    "Taxas de Entrada e Saída: Essas taxas são aplicadas ao preço inicial ao entrar em uma nova posição, simulando custos de transação. Isso torna o ambiente mais realista, pois o agente deve considerar os custos de negociação ao avaliar suas decisões.\n",
    "Exemplo de Uso\n",
    "Esse método pode ser usado a cada tick para calcular o retorno mais recente da posição atual do agente. Ele fornece uma métrica que o agente pode utilizar para atualizar sua recompensa ou avaliar o sucesso de uma ação tomada.\n",
    "\n",
    "Resumo\n",
    "O método most_recent_return é uma ferramenta importante para calcular os ganhos (ou perdas) mais recentes do agente, levando em consideração a posição (Long ou Short) e aplicando taxas de transação. Ele permite que o agente avalie o impacto de cada decisão de negociação, um aspecto essencial para o treinamento de um modelo de aprendizado por reforço em ambientes financeiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_portfolio_log_returns(self, action):\n",
    "    self.portfolio_log_returns[self._current_tick] = self.most_recent_return(\n",
    "        action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
