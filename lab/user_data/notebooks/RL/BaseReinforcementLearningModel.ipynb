{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import importlib\n",
    "import logging\n",
    "from abc import abstractmethod\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Optional, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.multiprocessing\n",
    "from pandas import DataFrame\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib.common.maskable.utils import is_masking_supported\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freqtrade.exceptions import OperationalException\n",
    "from freqtrade.freqai.data_kitchen import FreqaiDataKitchen\n",
    "from freqtrade.freqai.freqai_interface import IFreqaiModel\n",
    "from freqtrade.freqai.RL.Base5ActionRLEnv import Actions, Base5ActionRLEnv\n",
    "from freqtrade.freqai.RL.BaseEnvironment import BaseActions, BaseEnvironment, Positions\n",
    "from freqtrade.freqai.tensorboard.TensorboardCallback import TensorboardCallback\n",
    "from freqtrade.persistence import Trade\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SB3_MODELS = [\"PPO\", \"A2C\", \"DQN\"]\n",
    "SB3_CONTRIB_MODELS = [\"TRPO\", \"ARS\", \"RecurrentPPO\", \"MaskablePPO\", \"QRDQN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReinforcementLearningModel(IFreqaiModel):\n",
    "    \"\"\"\n",
    "    User created Reinforcement Learning Model prediction class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__(config=kwargs[\"config\"])\n",
    "        self.max_threads = min(\n",
    "            self.freqai_info[\"rl_config\"].get(\"cpu_count\", 1),\n",
    "            max(int(self.max_system_threads / 2), 1),\n",
    "        )\n",
    "        th.set_num_threads(self.max_threads)\n",
    "        self.reward_params = self.freqai_info[\"rl_config\"][\"model_reward_parameters\"]\n",
    "        self.train_env: Union[VecMonitor, SubprocVecEnv, gym.Env] = gym.Env()\n",
    "        self.eval_env: Union[VecMonitor, SubprocVecEnv, gym.Env] = gym.Env()\n",
    "        self.eval_callback: Optional[MaskableEvalCallback] = None\n",
    "        self.model_type = self.freqai_info[\"rl_config\"][\"model_type\"]\n",
    "        self.rl_config = self.freqai_info[\"rl_config\"]\n",
    "        self.df_raw: DataFrame = DataFrame()\n",
    "        self.continual_learning = self.freqai_info.get(\n",
    "            \"continual_learning\", False)\n",
    "        if self.model_type in SB3_MODELS:\n",
    "            import_str = \"stable_baselines3\"\n",
    "        elif self.model_type in SB3_CONTRIB_MODELS:\n",
    "            import_str = \"sb3_contrib\"\n",
    "        else:\n",
    "            raise OperationalException(\n",
    "                f\"{self.model_type} not available in stable_baselines3 or \"\n",
    "                f\"sb3_contrib. please choose one of {SB3_MODELS} or \"\n",
    "                f\"{SB3_CONTRIB_MODELS}\"\n",
    "            )\n",
    "\n",
    "        mod = importlib.import_module(import_str, self.model_type)\n",
    "        self.MODELCLASS = getattr(mod, self.model_type)\n",
    "        self.policy_type = self.freqai_info[\"rl_config\"][\"policy_type\"]\n",
    "        self.unset_outlier_removal()\n",
    "        self.net_arch = self.rl_config.get(\"net_arch\", [128, 128])\n",
    "        self.dd.model_type = import_str\n",
    "        self.tensorboard_callback: TensorboardCallback = TensorboardCallback(\n",
    "            verbose=1, actions=BaseActions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def unset_outlier_removal(self):\n",
    "        \"\"\"\n",
    "        If user has activated any function that may remove training points, this\n",
    "        function will set them to false and warn them\n",
    "        \"\"\"\n",
    "        if self.ft_params.get(\"use_SVM_to_remove_outliers\", False):\n",
    "            self.ft_params.update({\"use_SVM_to_remove_outliers\": False})\n",
    "            logger.warning(\"User tried to use SVM with RL. Deactivating SVM.\")\n",
    "        if self.ft_params.get(\"use_DBSCAN_to_remove_outliers\", False):\n",
    "            self.ft_params.update({\"use_DBSCAN_to_remove_outliers\": False})\n",
    "            logger.warning(\n",
    "                \"User tried to use DBSCAN with RL. Deactivating DBSCAN.\")\n",
    "        if self.ft_params.get(\"DI_threshold\", False):\n",
    "            self.ft_params.update({\"DI_threshold\": False})\n",
    "            logger.warning(\n",
    "                \"User tried to use DI_threshold with RL. Deactivating DI_threshold.\")\n",
    "        if self.freqai_info[\"data_split_parameters\"].get(\"shuffle\", False):\n",
    "            self.freqai_info[\"data_split_parameters\"].update(\n",
    "                {\"shuffle\": False})\n",
    "            logger.warning(\n",
    "                \"User tried to shuffle training data. Setting shuffle to False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, unfiltered_df: DataFrame, pair: str, dk: FreqaiDataKitchen, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Filter the training data and train a model to it. Train makes heavy use of the datakitchen\n",
    "        for storing, saving, loading, and analyzing the data.\n",
    "        :param unfiltered_df: Full dataframe for the current training period\n",
    "        :param metadata: pair metadata from strategy.\n",
    "        :returns:\n",
    "        :model: Trained model which can be used to inference (self.predict)\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\n",
    "            f\"--------------------Starting training {pair} --------------------\")\n",
    "\n",
    "        features_filtered, labels_filtered = dk.filter_features(\n",
    "            unfiltered_df,\n",
    "            dk.training_features_list,\n",
    "            dk.label_list,\n",
    "            training_filter=True,\n",
    "        )\n",
    "\n",
    "        dd: Dict[str, Any] = dk.make_train_test_datasets(\n",
    "            features_filtered, labels_filtered)\n",
    "        self.df_raw = copy.deepcopy(dd[\"train_features\"])\n",
    "        dk.fit_labels()  # FIXME useless for now, but just satiating append methods\n",
    "\n",
    "        # normalize all data based on train_dataset only\n",
    "        prices_train, prices_test = self.build_ohlc_price_dataframes(\n",
    "            dk.data_dictionary, pair, dk)\n",
    "\n",
    "        dk.feature_pipeline = self.define_data_pipeline(\n",
    "            threads=dk.thread_count)\n",
    "\n",
    "        (dd[\"train_features\"], dd[\"train_labels\"], dd[\"train_weights\"]) = (\n",
    "            dk.feature_pipeline.fit_transform(\n",
    "                dd[\"train_features\"], dd[\"train_labels\"], dd[\"train_weights\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.freqai_info.get(\"data_split_parameters\", {}).get(\"test_size\", 0.1) != 0:\n",
    "            (dd[\"test_features\"], dd[\"test_labels\"], dd[\"test_weights\"]) = (\n",
    "                dk.feature_pipeline.transform(\n",
    "                    dd[\"test_features\"], dd[\"test_labels\"], dd[\"test_weights\"]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\n",
    "            f'Training model on {len(dk.data_dictionary[\"train_features\"].columns)}'\n",
    "            f' features and {len(dd[\"train_features\"])} data points'\n",
    "        )\n",
    "\n",
    "        self.set_train_and_eval_environments(dd, prices_train, prices_test, dk)\n",
    "\n",
    "        model = self.fit(dd, dk)\n",
    "\n",
    "        logger.info(\n",
    "            f\"--------------------done training {pair}--------------------\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_train_and_eval_environments(\n",
    "        self,\n",
    "        data_dictionary: Dict[str, DataFrame],\n",
    "        prices_train: DataFrame,\n",
    "        prices_test: DataFrame,\n",
    "        dk: FreqaiDataKitchen,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        User can override this if they are using a custom MyRLEnv\n",
    "        :param data_dictionary: dict = common data dictionary containing train and test\n",
    "            features/labels/weights.\n",
    "        :param prices_train/test: DataFrame = dataframe comprised of the prices to be used in the\n",
    "            environment during training or testing\n",
    "        :param dk: FreqaiDataKitchen = the datakitchen for the current pair\n",
    "        \"\"\"\n",
    "        train_df = data_dictionary[\"train_features\"]\n",
    "        test_df = data_dictionary[\"test_features\"]\n",
    "\n",
    "        env_info = self.pack_env_dict(dk.pair)\n",
    "\n",
    "        self.train_env = self.MyRLEnv(\n",
    "            df=train_df, prices=prices_train, **env_info)\n",
    "        self.eval_env = Monitor(self.MyRLEnv(\n",
    "            df=test_df, prices=prices_test, **env_info))\n",
    "        self.eval_callback = MaskableEvalCallback(\n",
    "            self.eval_env,\n",
    "            deterministic=True,\n",
    "            render=False,\n",
    "            eval_freq=len(train_df),\n",
    "            best_model_save_path=str(dk.data_path),\n",
    "            use_masking=(self.model_type ==\n",
    "                         \"MaskablePPO\" and is_masking_supported(self.eval_env)),\n",
    "        )\n",
    "\n",
    "        actions = self.train_env.get_actions()\n",
    "        self.tensorboard_callback = TensorboardCallback(\n",
    "            verbose=1, actions=actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def pack_env_dict(self, pair: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create dictionary of environment arguments\n",
    "        \"\"\"\n",
    "        env_info = {\n",
    "            \"window_size\": self.CONV_WIDTH,\n",
    "            \"reward_kwargs\": self.reward_params,\n",
    "            \"config\": self.config,\n",
    "            \"live\": self.live,\n",
    "            \"can_short\": self.can_short,\n",
    "            \"pair\": pair,\n",
    "            \"df_raw\": self.df_raw,\n",
    "        }\n",
    "        if self.data_provider:\n",
    "            env_info[\"fee\"] = self.data_provider._exchange.get_fee(  # type: ignore\n",
    "                symbol=self.data_provider.current_whitelist()[0]\n",
    "            )\n",
    "\n",
    "        return env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @abstractmethod\n",
    "    def fit(self, data_dictionary: Dict[str, Any], dk: FreqaiDataKitchen, **kwargs):\n",
    "        \"\"\"\n",
    "        Agent customizations and abstract Reinforcement Learning customizations\n",
    "        go in here. Abstract method, so this function must be overridden by\n",
    "        user class.\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_state_info(self, pair: str) -> Tuple[float, float, int]:\n",
    "        \"\"\"\n",
    "        State info during dry/live (not backtesting) which is fed back\n",
    "        into the model.\n",
    "        :param pair: str = COIN/STAKE to get the environment information for\n",
    "        :return:\n",
    "        :market_side: float = representing short, long, or neutral for\n",
    "            pair\n",
    "        :current_profit: float = unrealized profit of the current trade\n",
    "        :trade_duration: int = the number of candles that the trade has\n",
    "            been open for\n",
    "        \"\"\"\n",
    "        open_trades = Trade.get_trades_proxy(is_open=True)\n",
    "        market_side = 0.5\n",
    "        current_profit: float = 0\n",
    "        trade_duration = 0\n",
    "        for trade in open_trades:\n",
    "            if trade.pair == pair:\n",
    "                if self.data_provider._exchange is None:  # type: ignore\n",
    "                    logger.error(\"No exchange available.\")\n",
    "                    return 0, 0, 0\n",
    "                else:\n",
    "                    current_rate = self.data_provider._exchange.get_rate(  # type: ignore\n",
    "                        pair, refresh=False, side=\"exit\", is_short=trade.is_short\n",
    "                    )\n",
    "\n",
    "                now = datetime.now(timezone.utc).timestamp()\n",
    "                trade_duration = int(\n",
    "                    (now - trade.open_date_utc.timestamp()) / self.base_tf_seconds)\n",
    "                current_profit = trade.calc_profit_ratio(current_rate)\n",
    "                if trade.is_short:\n",
    "                    market_side = 0\n",
    "                else:\n",
    "                    market_side = 1\n",
    "\n",
    "        return market_side, current_profit, int(trade_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(\n",
    "        self, unfiltered_df: DataFrame, dk: FreqaiDataKitchen, **kwargs\n",
    "    ) -> Tuple[DataFrame, npt.NDArray[np.int_]]:\n",
    "        \"\"\"\n",
    "        Filter the prediction features data and predict with it.\n",
    "        :param unfiltered_dataframe: Full dataframe for the current backtest period.\n",
    "        :return:\n",
    "        :pred_df: dataframe containing the predictions\n",
    "        :do_predict: np.array of 1s and 0s to indicate places where freqai needed to remove\n",
    "        data (NaNs) or felt uncertain about data (PCA and DI index)\n",
    "        \"\"\"\n",
    "\n",
    "        dk.find_features(unfiltered_df)\n",
    "        filtered_dataframe, _ = dk.filter_features(\n",
    "            unfiltered_df, dk.training_features_list, training_filter=False\n",
    "        )\n",
    "\n",
    "        dk.data_dictionary[\"prediction_features\"] = self.drop_ohlc_from_df(\n",
    "            filtered_dataframe, dk)\n",
    "\n",
    "        dk.data_dictionary[\"prediction_features\"], _, _ = dk.feature_pipeline.transform(\n",
    "            dk.data_dictionary[\"prediction_features\"], outlier_check=True\n",
    "        )\n",
    "\n",
    "        pred_df = self.rl_model_predict(\n",
    "            dk.data_dictionary[\"prediction_features\"], dk, self.model)\n",
    "        pred_df.fillna(0, inplace=True)\n",
    "\n",
    "        return (pred_df, dk.do_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def rl_model_predict(\n",
    "        self, dataframe: DataFrame, dk: FreqaiDataKitchen, model: Any\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        A helper function to make predictions in the Reinforcement learning module.\n",
    "        :param dataframe: DataFrame = the dataframe of features to make the predictions on\n",
    "        :param dk: FreqaiDatakitchen = data kitchen for the current pair\n",
    "        :param model: Any = the trained model used to inference the features.\n",
    "        \"\"\"\n",
    "        output = pd.DataFrame(np.zeros(len(dataframe)), columns=dk.label_list)\n",
    "\n",
    "        def _predict(window):\n",
    "            observations = dataframe.iloc[window.index]\n",
    "            if self.live and self.rl_config.get(\"add_state_info\", False):\n",
    "                market_side, current_profit, trade_duration = self.get_state_info(\n",
    "                    dk.pair)\n",
    "                observations[\"current_profit_pct\"] = current_profit\n",
    "                observations[\"position\"] = market_side\n",
    "                observations[\"trade_duration\"] = trade_duration\n",
    "            res, _ = model.predict(observations, deterministic=True)\n",
    "            return res\n",
    "\n",
    "        output = output.rolling(window=self.CONV_WIDTH).apply(_predict)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_ohlc_price_dataframes(\n",
    "        self, data_dictionary: dict, pair: str, dk: FreqaiDataKitchen\n",
    "    ) -> Tuple[DataFrame, DataFrame]:\n",
    "        \"\"\"\n",
    "        Builds the train prices and test prices for the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        pair = pair.replace(\":\", \"\")\n",
    "        train_df = data_dictionary[\"train_features\"]\n",
    "        test_df = data_dictionary[\"test_features\"]\n",
    "\n",
    "        # price data for model training and evaluation\n",
    "        tf = self.config[\"timeframe\"]\n",
    "        rename_dict = {\n",
    "            \"%-raw_open\": \"open\",\n",
    "            \"%-raw_low\": \"low\",\n",
    "            \"%-raw_high\": \" high\",\n",
    "            \"%-raw_close\": \"close\",\n",
    "        }\n",
    "        rename_dict_old = {\n",
    "            f\"%-{pair}raw_open_{tf}\": \"open\",\n",
    "            f\"%-{pair}raw_low_{tf}\": \"low\",\n",
    "            f\"%-{pair}raw_high_{tf}\": \" high\",\n",
    "            f\"%-{pair}raw_close_{tf}\": \"close\",\n",
    "        }\n",
    "\n",
    "        prices_train = train_df.filter(rename_dict.keys(), axis=1)\n",
    "        prices_train_old = train_df.filter(rename_dict_old.keys(), axis=1)\n",
    "        if prices_train.empty or not prices_train_old.empty:\n",
    "            if not prices_train_old.empty:\n",
    "                prices_train = prices_train_old\n",
    "                rename_dict = rename_dict_old\n",
    "            logger.warning(\n",
    "                \"Reinforcement learning module didn't find the correct raw prices \"\n",
    "                \"assigned in feature_engineering_standard(). \"\n",
    "                \"Please assign them with:\\n\"\n",
    "                'dataframe[\"%-raw_close\"] = dataframe[\"close\"]\\n'\n",
    "                'dataframe[\"%-raw_open\"] = dataframe[\"open\"]\\n'\n",
    "                'dataframe[\"%-raw_high\"] = dataframe[\"high\"]\\n'\n",
    "                'dataframe[\"%-raw_low\"] = dataframe[\"low\"]\\n'\n",
    "                \"inside `feature_engineering_standard()\"\n",
    "            )\n",
    "        elif prices_train.empty:\n",
    "            raise OperationalException(\n",
    "                \"No prices found, please follow log warning \"\n",
    "                \"instructions to correct the strategy.\"\n",
    "            )\n",
    "\n",
    "        prices_train.rename(columns=rename_dict, inplace=True)\n",
    "        prices_train.reset_index(drop=True)\n",
    "\n",
    "        prices_test = test_df.filter(rename_dict.keys(), axis=1)\n",
    "        prices_test.rename(columns=rename_dict, inplace=True)\n",
    "        prices_test.reset_index(drop=True)\n",
    "\n",
    "        train_df = self.drop_ohlc_from_df(train_df, dk)\n",
    "        test_df = self.drop_ohlc_from_df(test_df, dk)\n",
    "\n",
    "        return prices_train, prices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def drop_ohlc_from_df(self, df: DataFrame, dk: FreqaiDataKitchen):\n",
    "        \"\"\"\n",
    "        Given a dataframe, drop the ohlc data\n",
    "        \"\"\"\n",
    "        drop_list = [\"%-raw_open\", \"%-raw_low\", \"%-raw_high\", \"%-raw_close\"]\n",
    "\n",
    "        if self.rl_config[\"drop_ohlc_from_features\"]:\n",
    "            df.drop(drop_list, axis=1, inplace=True)\n",
    "            feature_list = dk.training_features_list\n",
    "            dk.training_features_list = [\n",
    "                e for e in feature_list if e not in drop_list]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_model_from_disk(self, dk: FreqaiDataKitchen) -> Any:\n",
    "        \"\"\"\n",
    "        Can be used by user if they are trying to limit_ram_usage *and*\n",
    "        perform continual learning.\n",
    "        For now, this is unused.\n",
    "        \"\"\"\n",
    "        exists = Path(dk.data_path / f\"{dk.model_filename}_model\").is_file()\n",
    "        if exists:\n",
    "            model = self.MODELCLASS.load(\n",
    "                dk.data_path / f\"{dk.model_filename}_model\")\n",
    "        else:\n",
    "            logger.info(\"No model file on disk to continue learning from.\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _on_stop(self):\n",
    "        \"\"\"\n",
    "        Hook called on bot shutdown. Close SubprocVecEnv subprocesses for clean shutdown.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_env:\n",
    "            self.train_env.close()\n",
    "\n",
    "        if self.eval_env:\n",
    "            self.eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Nested class which can be overridden by user to customize further\n",
    "    class MyRLEnv(Base5ActionRLEnv):\n",
    "        \"\"\"\n",
    "        User can override any function in BaseRLEnv and gym.Env. Here the user\n",
    "        sets a custom reward based on profit and trade duration.\n",
    "        \"\"\"\n",
    "\n",
    "        def calculate_reward(self, action: int) -> float:  # noqa: C901\n",
    "            \"\"\"\n",
    "            An example reward function. This is the one function that users will likely\n",
    "            wish to inject their own creativity into.\n",
    "\n",
    "            Warning!\n",
    "            This is function is a showcase of functionality designed to show as many possible\n",
    "            environment control features as possible. It is also designed to run quickly\n",
    "            on small computers. This is a benchmark, it is *not* for live production.\n",
    "\n",
    "            :param action: int = The action made by the agent for the current candle.\n",
    "            :return:\n",
    "            float = the reward to give to the agent for current step (used for optimization\n",
    "                of weights in NN)\n",
    "            \"\"\"\n",
    "            # first, penalize if the action is not valid\n",
    "            if not self._is_valid(action):\n",
    "                return -2\n",
    "\n",
    "            pnl = self.get_unrealized_profit()\n",
    "            factor = 100.0\n",
    "\n",
    "            # you can use feature values from dataframe\n",
    "            rsi_now = self.raw_features[\n",
    "                f\"%-rsi-period-10_shift-1_{self.pair}_{self.config['timeframe']}\"\n",
    "            ].iloc[self._current_tick]\n",
    "\n",
    "            # reward agent for entering trades\n",
    "            if (\n",
    "                action in (Actions.Long_enter.value, Actions.Short_enter.value)\n",
    "                and self._position == Positions.Neutral\n",
    "            ):\n",
    "                if rsi_now < 40:\n",
    "                    factor = 40 / rsi_now\n",
    "                else:\n",
    "                    factor = 1\n",
    "                return 25 * factor\n",
    "\n",
    "            # discourage agent from not entering trades\n",
    "            if action == Actions.Neutral.value and self._position == Positions.Neutral:\n",
    "                return -1\n",
    "\n",
    "            max_trade_duration = self.rl_config.get(\n",
    "                \"max_trade_duration_candles\", 300)\n",
    "            if self._last_trade_tick:\n",
    "                trade_duration = self._current_tick - self._last_trade_tick\n",
    "            else:\n",
    "                trade_duration = 0\n",
    "\n",
    "            if trade_duration <= max_trade_duration:\n",
    "                factor *= 1.5\n",
    "            elif trade_duration > max_trade_duration:\n",
    "                factor *= 0.5\n",
    "\n",
    "            # discourage sitting in position\n",
    "            if (\n",
    "                self._position in (Positions.Short, Positions.Long)\n",
    "                and action == Actions.Neutral.value\n",
    "            ):\n",
    "                return -1 * trade_duration / max_trade_duration\n",
    "\n",
    "            # close long\n",
    "            if action == Actions.Long_exit.value and self._position == Positions.Long:\n",
    "                if pnl > self.profit_aim * self.rr:\n",
    "                    factor *= self.rl_config[\"model_reward_parameters\"].get(\n",
    "                        \"win_reward_factor\", 2)\n",
    "                return float(pnl * factor)\n",
    "\n",
    "            # close short\n",
    "            if action == Actions.Short_exit.value and self._position == Positions.Short:\n",
    "                if pnl > self.profit_aim * self.rr:\n",
    "                    factor *= self.rl_config[\"model_reward_parameters\"].get(\n",
    "                        \"win_reward_factor\", 2)\n",
    "                return float(pnl * factor)\n",
    "\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(\n",
    "    MyRLEnv: Type[BaseEnvironment],\n",
    "    env_id: str,\n",
    "    rank: int,\n",
    "    seed: int,\n",
    "    train_df: DataFrame,\n",
    "    price: DataFrame,\n",
    "    env_info: Dict[str, Any] = {},\n",
    ") -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the initial seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :param env_info: (dict) all required arguments to instantiate the environment.\n",
    "    :return: (Callable)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _init() -> gym.Env:\n",
    "        env = MyRLEnv(df=train_df, prices=price, id=env_id,\n",
    "                      seed=seed + rank, **env_info)\n",
    "\n",
    "        return env\n",
    "\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
